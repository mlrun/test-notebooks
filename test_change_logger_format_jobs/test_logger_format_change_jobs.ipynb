{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c7db8d2-4b60-42e3-a97b-6fa3349d2331",
   "metadata": {},
   "source": [
    "#  Test change mlrun log formmater as environment variable for jobs runs\n",
    "\n",
    "realted to - https://iguazio.atlassian.net/browse/ML-5763\n",
    "\n",
    "This Notebook checks a use case that if a user add MLRUN_LOG_FORMATTER=value environment variable, the format of the logger for the specific run will change to the environment variable value, for example if i set thie environment variable MLRUN_LOG_FORMATTER='json' the logger will be in json format.\n",
    "\n",
    "Currently we have 3 options for logger format:\n",
    "* human - default value\n",
    "* human_extended\n",
    "* json \n",
    "\n",
    "the test will pass if the mlrun.config.log_formatter value is equal to the one in the environment variable and the logger handler format is equal the expcted formatter object in MLRun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2696438-e6b5-4cf3-b9c0-6bb6c541b198",
   "metadata": {},
   "source": [
    "#### Creathing a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e1ec4-e7f3-4eec-8e3b-a2d3018cf031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26c84ab-5b26-42da-94e3-834628e0bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = mlrun.get_or_create_project(\"test-logger\", context=\"./test-logger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64186b60-2d7e-4ac9-b256-5f7af3e81834",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test human format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff18ee58-e395-4339-bb3f-6c911a68e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./test-logger/func_with_set_logger_json.py\n",
    "import mlrun\n",
    "def func(context):\n",
    "    context.logger.set_logger_level(level=\"WARN\")\n",
    "    context.logger.error(\"ERROR\")\n",
    "    context.logger.warn(\"WARN\")\n",
    "    context.logger.debug(\"DEBUG\")\n",
    "    return mlrun.mlconf.log_formatter , type(context.logger.get_handler(\"default\").formatter) == mlrun.utils.HumanReadableFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93ff53-38b2-4e7f-956c-a03695a7f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_func_human = project.set_function(name=\"human-func\",handler=\"func\",func=\"func_with_set_logger_json.py\",image=\"mlrun/mlrun\",kind='job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e88ee-6e35-410c-81e2-cce4671c888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_func_human.set_env(name=\"MLRUN_LOG_FORMATTER\",value=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55502a-6464-4b3f-adb4-dfaa98a27117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_human = test_func_human.run(returns=[\"mlrun_mlconf\",\"formatter_bool_check\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4a3560-7180-4214-b86b-134d92a1834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert run_human.outputs[\"mlrun_mlconf\"]=='human' and run_human.outputs[\"formatter_bool_check\"] == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a31d8-b3d9-4b83-acdf-700caee72b5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test human extended format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46849d43-d777-462f-9992-8228e5016426",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./test-logger/func_with_set_logger_json.py\n",
    "import mlrun\n",
    "def func(context):\n",
    "    context.logger.set_logger_level(level=\"WARN\")\n",
    "    context.logger.error(\"ERROR\")\n",
    "    context.logger.warn(\"WARN\")\n",
    "    context.logger.debug(\"DEBUG\")\n",
    "    return mlrun.mlconf.log_formatter , type(context.logger.get_handler(\"default\").formatter) == mlrun.utils.HumanReadableExtendedFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e7177e-e383-40f6-88c3-8ea59a5745c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_func_human_extended = project.set_function(name=\"human-extended-func\",handler=\"func\",func=\"func_with_set_logger_json.py\",image=\"mlrun/mlrun\",kind='job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1eb8dd-9a3f-467a-852c-c4aeaaeb4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_func_human_extended.set_env(name=\"MLRUN_LOG_FORMATTER\",value=\"human_extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a6bd3-6405-44c6-bbb1-1b663b54742f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_human_extended = test_func_human_extended.run(returns=[\"mlrun_mlconf\",\"formatter_bool_check\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311947e7-35b3-47f3-8f01-980745cf91dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert run_human_extended.outputs[\"mlrun_mlconf\"]=='human_extended' and run_human_extended.outputs[\"formatter_bool_check\"] == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52425cfb-19c0-434e-9e91-fe9c59d20f70",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test human format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953cf5f2-af68-49c3-b0d7-d72dfc3bf892",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./test-logger/func_with_set_logger_json.py\n",
    "import mlrun\n",
    "def func(context):\n",
    "    context.logger.set_logger_level(level=\"WARN\")\n",
    "    context.logger.error(\"ERROR\")\n",
    "    context.logger.warn(\"WARN\")\n",
    "    context.logger.debug(\"DEBUG\")\n",
    "    return mlrun.mlconf.log_formatter , type(context.logger.get_handler(\"default\").formatter) == mlrun.utils.JSONFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d9500-944b-4861-9833-68cafa08408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_func_json = project.set_function(name=\"json-func\",handler=\"func\",func=\"func_with_set_logger_json.py\",image=\"mlrun/mlrun\",kind='job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed95558c-fc33-43db-a239-a73fc161b944",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_func_json.set_env(name=\"MLRUN_LOG_FORMATTER\",value=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef836052-aaef-4bb7-a6a4-66baca9e9434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_json = test_func_json.run(returns=[\"mlrun_mlconf\",\"formatter_bool_check\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3cb6c2-f8b1-4c23-b394-ded3c2cdd3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert run_json.outputs[\"mlrun_mlconf\"]=='json' and run_json.outputs[\"formatter_bool_check\"] == True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-base",
   "language": "python",
   "name": "conda-env-mlrun-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
