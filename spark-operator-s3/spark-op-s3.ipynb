{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLRun Spark on K8s Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using awscli==1.35.2 due to ML-9923"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!version=`curl -s ${IGZ_MLRUN_API_ENDPOINT}/api/v1/client-spec | python3 -c \"import sys, json; print(json.load(sys.stdin)['version'])\"`; pip install awscli==1.35.2 mlrun[complete]==$version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create or load a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the MLRun project object\n",
    "import mlrun\n",
    "import os\n",
    "\n",
    "project_name = \"mlrun-spark-k8s\"\n",
    "project = mlrun.get_or_create_project(project_name, context=\"./\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credentials & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "work_dir = str(uuid.uuid4())\n",
    "\n",
    "secrets = {'AWS_ACCESS_KEY_ID': os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "           'AWS_SECRET_ACCESS_KEY': os.getenv('AWS_SECRET_ACCESS_KEY')}\n",
    "\n",
    "S3_BUCKET = os.environ.get('S3_BUCKET', 'testbucket-igz-temp')\n",
    "\n",
    "source_path = f\"{S3_BUCKET}/{work_dir}/dataset.csv\"\n",
    "target_path = f\"{S3_BUCKET}/{work_dir}/target-pq\"\n",
    "\n",
    "# Create project secrets with S3 credentials\n",
    "project.set_secrets(secrets=secrets, provider=\"kubernetes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload dataset to S3\n",
    "!aws s3 cp dataset.csv s3://$source_path\n",
    "\n",
    "# cleanup from previous runs\n",
    "!aws s3 rm --recursive s3://$target_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy default spark image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun.runtimes import Spark3Runtime\n",
    "Spark3Runtime.deploy_default_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate an MLRun function of kind Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = project.set_function(name=\"my-spark-function\", tag=\"latest\",\n",
    "                          func=\"simple-spark-etl.py\", kind=\"spark\",\n",
    "                          image='.spark-job-default-image:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the function's resources\n",
    "fn.with_driver_limits(cpu=\"1300m\")\n",
    "fn.with_driver_requests(cpu=1, mem=\"1300m\")\n",
    "fn.with_executor_limits(cpu=\"1400m\")\n",
    "fn.with_executor_requests(cpu=1, mem=\"512m\")\n",
    "fn.spec.replicas = 2\n",
    "fn.spec.image_pull_policy = 'Always'\n",
    "\n",
    "\n",
    "# set arguments\n",
    "fn.spec.args=['--source_path',f\"s3a://{source_path}\", '--target_path',f\"s3a://{target_path}\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.run_function(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = !aws s3 ls s3://$target_path/ | wc -l\n",
    "assert count == ['21'] or count == ['22']  # newer hadoop / aws jar can add extra empty \"0\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://$target_path/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 rm --recursive s3://$target_path\n",
    "!aws s3 rm --recursive s3://$source_path\n",
    "\n",
    "mlrun.get_run_db().delete_project(name=project.name, deletion_strategy='cascade')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-base",
   "language": "python",
   "name": "conda-env-mlrun-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
